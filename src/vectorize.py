import numpy as np
import pandas as pd

import re

import spacy
from spacy.language import Language
from spacy.lang.en.stop_words import STOP_WORDS

from collections import Counter


def clean_html(raw_html: str) -> (str):
    '''Accepts a string of text and removes characters according to the 
    pattern. Then, the string is split into words and unwanted characters 
    are removed from each word. The word is added to the list clean_word,
    and then clean_words filtered for web addresses/non-alphabetic strings.
    A string of the elements in clean_words is returned.
    '''
    
    pattern = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')
    clean_text = re.sub(pattern, '', raw_html)
    
    clean_words = []
    for word in clean_text.split(' '):
        w = re.sub(r'[^\w\s]',' ',word)
        w = w.replace(r'.*</p>\.', '.* ')
        clean_words.append(w.strip())

    return ' '.join(clean_words)


def clean_lemmas(docs: list) -> (list):
    '''Accepts a list of spacy docs and returns a modified list rmeoving
    digits, punctuation, stop words, and short words (len <= 3),
    '''
    return [[n.lemma_ for n in doc 
            if n.is_digit == False
            and n.is_punct == False
            and n.is_stop ==False
            and len(str(n)) >= 3]
            for doc in docs]


def embed(text_vectors: list, word: str) -> (float):
    '''Accepts a list of tuples containing a text(str) and vector(float),
    and a word(str). The vector(float) is returned if word appears in 
    text_vectors.
    '''
    vector = [tv[1] for tv in text_vectors 
              if str(tv[0]) == word]   
    if len(vector) > 0:
        return vector[0]
    else:
        return 0
    

def create_vectors(docs: list, vectors: list) -> (list):
    '''Accepts a docs(list) and vectors(list), each doc(spacy doc) in docs 
    corresponds to a vec(list) in vectors. Each d(spacy token) in doc is
    represented by a v(spacy vector) in the corresponding vec. docs and
    vecs are zipped together to create doc_vecs. Each doc_vec in doc_vecs
    is zipped together and each token is stored in tuple with its vector.
    Returns a list of tuples.    
    '''
    doc_vecs = list(zip(docs, vectors))
    doc_vecs_flat = []
    for dv in doc_vecs:
        dv_flat = list(zip(dv[0], dv[1]))
        doc_vecs_flat.append(dv_flat)
        
    return doc_vecs_flat


def filter_doc_vecs(doc_vecs_flat: list) -> (list):
    '''Accepts doc_vecs_flat(list of tuples) containing tokens and their
    vectors. A modified list of tokens is returned, removing stop words,
    digits, and small words.
    '''
    return [[d for d in dv
            if d[0].is_stop == False
            and d[0].is_digit == False
            and len(str(d[0])) >= 3]
            for dv in doc_vecs_flat]


def filter_vocab(vocab: list, threshold: int) -> (list):
    '''Accepts a vocab(list of strings) and threshold(int) and counts the
    frequency of each word in vocab using Counter and ordered by frequency. 
    The resulting list is filtered for words with frequencies above the 
    threshold. Returns a list of those words filtered for duplicates.
    '''
    counted = Counter(vocab).most_common()
    counted = [c for c in counted if c[1] >= threshold]
    top_words = [c[0] for c in counted]
    return list(set([v.lower() for v in vocab if v in top_words]))


def vectorize_data(data: pd.DataFrame) -> (pd.DataFrame):
    '''Accepts data(pd.DataFrame), filtered_dv_flat(list) and vocab(list).
    A column in data is created for filtered_dv_flat. A list, filtered_vocab
    is generated by applying filter_vocab to vocab. A new column in data is
    created for each word in filtered_vocab by applying embed to dv_flat.
    Columns not needed are dropped and labels are binarized. The vectorized
    dataframe is returned.
    '''
    
    nlp = spacy.load('en_core_web_md')
    data['nlp_data'] = data['message'].apply(lambda x: clean_html(x))
    docs = [nlp(doc) for doc in data['nlp_data']]
    vectors = [doc.vector for doc in docs]
    doc_vecs_flat = create_vectors(docs, vectors)
    filtered_dv_flat = filter_doc_vecs(doc_vecs_flat)

    data['dv_flat'] = filtered_dv_flat
    vocab = [str(d[0]).lower() for dv in filtered_dv_flat for d in dv]

    filtered_vocab = [word for word in filter_vocab(vocab, 10) 
                      if len(word) > 3]
    
    for word in filtered_vocab:
        data[word] = data['dv_flat'].apply(lambda x: embed(x, word))
        
    drop = [col for col in data.columns 
            if col in ['message', 'nlp_data', 'dv_flat']]
    
    vectorized_data = data.drop(drop, axis=1)
    vectorized_data['label'] = vectorized_data['label'].apply(lambda x:
                                                        1 if x == 'lib' 
                                                        else 0)
    return vectorized_data